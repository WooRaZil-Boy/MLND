{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Deep Q-Network: Reinforcement Learning with TensorFlow&OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 1. Basic Environment\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://gym.openai.com/). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the FrozenLake game. \n",
    "\n",
    "FrozenLake is that a agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-06 22:27:25,865] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym #Import 'OpenAIGym'\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\") #Make basic environment\n",
    "observation = env.reset() #Initialize the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`. To show the simulation running, you can use `env.render()` to render one frame. Passing in an action as an integer to `env.step` will generate the next step in the simulation.  You can see how many actions are possible from `env.action_space` and to get a random action you can use `env.action_space.sample()`. In the FrozenLake game, there are 4 possible actions, moving the agent up, down, left or right. And there are two rewards we can take, encoded as 0 and 1.\n",
    "\n",
    "* env.render() : display envoronment\n",
    "* env.action_space.sample() : the agent action\n",
    "* env.step() : execute step and get infomations\n",
    "\n",
    "![frozen_lake_agent](images/frozen_lake_agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render() #Display environment\n",
    "action = env.action_space.sample() #The agent action.\n",
    "observation, reward, done, info = env.step(action) #Execute step and get infomations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 4x4 grid environment.      \n",
    "    SFFF       (S: starting point, safe)    \n",
    "    FHFH       (F: frozen surface, safe)    \n",
    "    FFFH       (H: hole, fall to your doom)    \n",
    "    HFFG       (G: goal, where the frisbee is located)    \n",
    "    \n",
    "There is the FrozenLake map that we made. \n",
    "\n",
    "![frozen_lake_1](images/frozen_lake_1.png)\n",
    "\n",
    "In this game, the episode ends when you reach the goal or fall in a hole. The agent receive a reward of 1 if it reach the goal, and zero otherwise. So, our final goal is that we get a reward of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 2. Basic Q-table\n",
    "\n",
    "But, We are stucked. Because we don't know where is holes. There is the real FrozenLake map for our agent.\n",
    "\n",
    "![frozen_lake_2](images/frozen_lake_2.png)\n",
    "\n",
    "We can train the agent by random actions. But, as you know, it isn't efficient. So, we need other solution. The solution is that:  \n",
    "> Even if you know the way, **ask** one more time. - Korean proverbs\n",
    "\n",
    "Before we action, the agent should ask to someone. This is [Q-learning](https://en.wikipedia.org/wiki/Q-learning). Q-learning is a model-free reinforcement learning technique. Specifically, Q-learning can be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP). It works by learning an action-value function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter. A policy is a rule that the agent follows in selecting actions, given the state it is in. When such an action-value function is learned, the optimal policy can be constructed by simply selecting the action with the highest value in each state. One of the strengths of Q-learning is that it is able to compare the expected utility of the available actions without requiring a model of the environment. Additionally, Q-learning can handle problems with stochastic transitions and rewards, without requiring any adaptations. It has been proven that for any finite MDP, Q-learning eventually finds an optimal policy, in the sense that the expected value of the total reward return over all successive steps, starting from the current state, is the maximum achievable. \n",
    "\n",
    "There is a simple Q-learning function. If we give the state and action as parameters, the function will return optimized value. \n",
    "\n",
    "![q_table](images/q_table.png)\n",
    "\n",
    "Here is an example. Suppose the agent is on the 's1'.\n",
    "\n",
    "| Q-function: Q(state, action) \t| reward \t|\n",
    "|:-------:\t|:------:\t|\n",
    "|   Q(s1, Up)  \t|  0   |\n",
    "|   Q(s1, Down)   \t|  0.3  |\n",
    "|   Q(s1, Left)   \t| 0  |\n",
    "|   Q(s1, Right)   \t|  0.5  |\n",
    "\n",
    "This table shows reward according to each actions on the 's1'. We can choose a action that gives the greatest reward. In this case, it is 'Right'. So, here is a equation:\n",
    "\n",
    "$$\n",
    "\\pi^{*}{(s)} = {\\operatorname{argmax}}{Q(s, a)}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $\\pi$ is a policy, $^{*}$ is meaning of optimization. So, if $r$ is a reward, we can calculate sum of rewards:\n",
    "\n",
    "$$\n",
    "R = r_1 + r_2 + r_3 + \\dots + r_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_t = r_t + r_{t+1} + r_{t+2} + \\dots + r_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_t = r_t + R_{t+1}\n",
    "$$\n",
    "\n",
    "When $s'$ is the next state from state $s$ and action $a$. We should assume ${Q(s', a')}$ is exist. So, We train our Q-learning agent using the equation:\n",
    "\n",
    "$$\n",
    "{Q(s, a)} = r + \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "\n",
    "![q_table_equation](images/q_table_equation.png)\n",
    "\n",
    "Finally, we get a optimize function ${Q(s, a)} = r + \\max{Q(s', a')}$. Let's dive in to the deep. First of all, we should initialize by zero.\n",
    "\n",
    "![q_table_init](images/q_table_init.png)\n",
    "\n",
    "And we should update our Q-table by many trials. The agent can recieve a reward of 1 only when move to the right on $s_{14}$. So, before the agent arrives on $s_{14}$, we couldn't get any reward(always get a reward of zero). And return value of Q-function is also zero. So, we shouldn't update any value. But, after many trials, let's assume that the agent is on $s_{14}$. In this case, finally we can get a reward. because ${Q(s_{14}, a_{right})}$ will return a reward of 1. And, $\\max{(Q(s_{15}, a))}$ is 0. Because, $s_{15}$ cell is our goal. So, we should update ${Q(s_{14}, a_{right})} = 1$.\n",
    "\n",
    "$$\n",
    "{Q(s, a)} = r + \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{Q(s_{14}, a_{right})} = 1 + 0 = 1\n",
    "$$\n",
    "\n",
    "![q_table_reward](images/q_table_reward.png)\n",
    "\n",
    "After $s_{14}$ state was updated, let's assume that the agent is on $s_{13}$ like before example($s_{14}$). In this case, ${Q(s_{13}, a_{right})}$ couldn't get a reward. But, $\\max{Q(s', a')}$ is 1. Because, $s'$ is $s_{14}$, so, $\\max{Q(s', a')}$ is ${Q(s_{14}, a_{right})}$. So, we should update ${Q(s_{13}, a_{right})} = 1$\n",
    "\n",
    "$$\n",
    "{Q(s, a)} = r + \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{Q(s_{13}, a_{right})} = r + \\max{Q(s_{14}, a')} = 0 + {Q(s_{14}, a_{right})} = 1\n",
    "$$\n",
    "\n",
    "![q_table_reward_2](images/q_table_reward_2.png)\n",
    "\n",
    "In conclusion, we can summarize our first algorithm.\n",
    "\n",
    "* For each $s$, $a$ initialize table entry $\\hat{Q}(s, a) \\leftarrow 0$. \n",
    "* Observe current state $s$\n",
    "* Do forever:\n",
    "    * Select an action $a$ and execute it\n",
    "    * Receive immediate reward $r$\n",
    "    * Observe the new state $s'$\n",
    "    * Update the table entry for $\\hat{Q}(s, a)$ as follows:        \n",
    "        $\\hat{Q}(s, a) \\leftarrow r + \\max\\hat{Q}(s', a')$\n",
    "    * $s \\leftarrow s'$\n",
    "    \n",
    "$\\hat{Q}$ denote learner's current approximation to $Q$. According to [Tom Mitchell](https://www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_2?ie=UTF8&qid=1501476742&sr=8-2&keywords=tom+mitchell+machine+learning), $\\hat{Q}$ converge to $Q$ when it is in deterministic worlds and in finite states.\n",
    "    \n",
    "Time to implement our first Q-learning. First of all, we should set FrozenLake environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-29 12:32:12,150] Making new env: FrozenLake-v3\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.registration import register\n",
    "import random as pr\n",
    "\n",
    "def rargmax(vector): #https://gist.github.com/stober/1943451\n",
    "    m = np.amax(vector) #Return the maximum of an array or maximum along an axis.\n",
    "    indices = np.nonzero(vector == m)[0] #Return the indices of the elements that are non-zero.\n",
    "    #x = np.array([[1,0,0], [0,2,0], [1,1,0]])\n",
    "    #np.nonzero(x) : (array([0, 1, 2, 2]), array([0, 1, 0, 1]))\n",
    "    #[0, 0], [1, 1], [2, 0], [2, 1] are 'nonzero'\n",
    "\n",
    "    return pr.choice(indices) \n",
    "    #If there are duplicate values, Nunmpy returns the first value. \n",
    "    #So, we use random.choice to return random index\n",
    "\n",
    "register(\n",
    "    id='FrozenLake-v1',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4',\n",
    "            'is_slippery': False}\n",
    ") #Environment setting\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\") #Environment generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, Let's implement the Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n]) #16, 4(4x4 : each state, [Up, Down, Left, Right])\n",
    "\n",
    "#Set learning parameters\n",
    "num_episodes = 2000 #Repeat 2000 times\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "rList = [] #List for saving result\n",
    "\n",
    "for i in range(num_episodes): \n",
    "    # Reset environment and get first new observation\n",
    "    state = env.reset() #Initialize evironment \n",
    "    rAll = 0 #Result. When this value is 0, it means the agent fall in a hole. \n",
    "             #And when this value is 1, it means the agent reach the goal.\n",
    "    done = False \n",
    "\n",
    "    #The Q-learning algorithm with table\n",
    "    while not done:\n",
    "        action = rargmax(Q[state, :]) #parameter : Q[current state, all of actions(Up, Down, Left, Right)]\n",
    "        new_state, reward, done, _ = env.step(action) #Get new state and reward from environment\n",
    "        Q[state, action] = reward + np.max(Q[new_state, :]) #Update table with new knowledge using learning rate\n",
    "        rAll += reward #Sum up the reward.\n",
    "        state = new_state #state update (the agent moves to next state)\n",
    "        \n",
    "    #After the game is over. (while loop finished)\n",
    "    rList.append(rAll) #Append result to the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to result, after learning, we can see that the agent always succeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.8295\n",
      "Final Q-Table Values\n",
      "LEFT DOWN RIGHT UP\n",
      "[[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1KD/B+G3NzJ4YAQPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119ffd7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Success rate: \" + str(sum(rList) / num_episodes))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(\"LEFT DOWN RIGHT UP\")\n",
    "print(Q)\n",
    "plt.bar(range(len(rList)), rList, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the map that updated all the states.\n",
    "\n",
    "![frozen_lake_3](images/frozen_lake_3.png)\n",
    "\n",
    "But, we can find a problem. The problem is that the agent's route isn't optimized.\n",
    "\n",
    "![frozen_lake_finish_1](images/frozen_lake_finish_1.png)\n",
    "\n",
    "Our route is red line. But, optimized route is green line. Although we got a correct reward of 1, it isn't efficient route. In our case, to learn about the environment and rules of the game, the agent needs to explore by taking random actions even though these actions haven't optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 3. Optimized Q-table with Exploit and Exploration\n",
    "\n",
    "Why isn't the first algorithm optimized? Let's see the algorithm again:\n",
    "\n",
    "$$\n",
    "\\pi^{*}{(s)} = {\\operatorname{argmax}}{Q(s, a)}\n",
    "$$\n",
    "\n",
    "where $s$ is a state, $a$ is an action, and $\\pi$ is a policy, $^{*}$ is meaning of optimization. According to this equation, the agent should move to next state that has the greatest value. So, although other route has more efficient way, the agent always moves in the same way. This is the problem of our first algorithm.\n",
    "\n",
    "To improve our algorithm, although other state has small value, sometimes the agent should move to there. Let's call this method as **Exploit VS Exploration**. A simple key of this strategy is that sometimes the agent should move randomly. Here is a example in real life. I'm a big fan of Burger King Whopper. And I like McDonald's Big Mac too. But, I don't like KFC and Popeyes. If I express this as a score, here is my preference of food chains.\n",
    "\n",
    "![junk_food_reward](images/junk_food_reward.png)\n",
    "\n",
    "Even though I want Big Mac to eat sometimes, according to our first algorithm, I will always go to Burger King. Because Burger King has the greatest preference value. And, I can't find a new delicious hamburger in other food chains. So, I decided to change my strategy. I will go to Burger King in weekdays, because it is my best food chain **(Exploit)**. But I will go to other food chain in weekend to find a new delicious hamburger **(Exploration)**. There are two ways to improve our strategy. \n",
    "\n",
    "The first is called an **$\\epsilon$-greedy policy**. The main concept of this method is that set a small probability. Let's call this probability $\\epsilon$ (epsilon). The agent will choose random action with $\\epsilon$ probability. Contrary, it will choose an action from ${Q(s, a)}$ with $1 - \\epsilon$ probability.\n",
    "\n",
    "```\n",
    "e = 0.1\n",
    "if rand < e:\n",
    "    a = random\n",
    "else:\n",
    "    a = argmax(Q(s, a))\n",
    "```\n",
    "\n",
    "So, I will go to Burger King with a 90% chance. And I will go to other food chains with 10% chance. Like this, the agent will follow ${Q(s, a)}$ with 90% chance. And the agent will find new way with a 10% chance. But, this strategy isn't efficient. Because, as learning progresses, the agent need to find a new way less and less. So, at first, the agent needs to do a lot of exploring. Later when it has learned more, the agent can favor choosing actions based on what it has learned. This is called exploitation. We'll set it up so the agent is more likely to explore early in training, then more likely to exploit later in training. The is called an **decaying $\\epsilon$-greedy policy**\n",
    "\n",
    "```\n",
    "for i in range(1000):\n",
    "    e = 0.1 / (i+1)\n",
    "    \n",
    "    if random(1) < e:\n",
    "        a = random\n",
    "    else:\n",
    "        a = argmax(Q(s, a)) \n",
    "```\n",
    "\n",
    "Accorind to this, as time passes, $\\epsilon$ is decreased. So, later when it has learned more, the agent can choose action based on what it has learned than random action.\n",
    "\n",
    "And the second is called an **add random noise**. The concept of this method is that just add random value to each action. \n",
    "\n",
    "![junk_food_random_value](images/junk_food_random_value.png)\n",
    "\n",
    "$$\n",
    "{a} = {\\operatorname{argmax}}{Q(s, a) + {RandomValue}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{a} = {\\operatorname{argmax}}{([0.3, 0.9, 0.2, 0.7] + [0.2, 0.1, 0.4, 0.5])}\n",
    "$$\n",
    "\n",
    "In this case, we will choose McDonald's. And also we can use dacaying policy with add random noise.\n",
    "\n",
    "```\n",
    "for i in range(1000):\n",
    "    a = argmax(Q(s, a) + random_value / (i+1))\n",
    "```\n",
    "\n",
    "Comparing **$\\epsilon$-greedy policy** and **add random noise**, when the agent choose random action in $\\epsilon$-greedy policy, it is totally random. But same case in add random noise, since the noise_values  are added to the existing $Q(s, a)$ values, it is affected by Q-learning function value even though the agent will choose random action. So, the probability that an action has a high Q-leaning value is selected is higher than $\\epsilon$-greedy policy. Let's check our first algorithm again:\n",
    "\n",
    "* For each $s$, $a$ initialize table entry $\\hat{Q}(s, a) \\leftarrow 0$. \n",
    "* Observe current state $s$\n",
    "* Do forever:\n",
    "    * **Select an action $a$ and execute it**\n",
    "    * Receive immediate reward $r$\n",
    "    * Observe the new state $s'$\n",
    "    * Update the table entry for $\\hat{Q}(s, a)$ as follows:        \n",
    "        $\\hat{Q}(s, a) \\leftarrow r + \\max\\hat{Q}(s', a')$\n",
    "    * $s \\leftarrow s'$\n",
    "\n",
    "In this algorithm, we didn't tell how to select an action and execute it. Now, the agent will select an action with **$\\epsilon$-greedy policy** or **add random noise**. Now, let's implement the Q-learning with add random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.902\n",
      "Final Q-Table Values\n",
      "[[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npAYZ7pLUoP8Ha4o9OEn2vcYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b49c6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n]) #16, 4(4x4 : each state, [Up, Down, Left, Right])\n",
    "\n",
    "#Set learning parameters\n",
    "num_episodes = 2000 #Repeat 2000 times\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "rList = [] #List for saving result\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # Reset environment and get first new observation\n",
    "    state = env.reset() #Initialize evironment \n",
    "    rAll = 0 #Result. When this value is 0, it means the agent fall in a hole. \n",
    "             #And when this value is 1, it means the agent reach the goal.\n",
    "    done = False \n",
    "\n",
    "    # The Q-Table learning algorithm\n",
    "    while not done:\n",
    "        # Choose an action by greedily (with noise) picking from Q table\n",
    "        action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i + 1)) #Add random noise.\n",
    "        #parameter : Q[current state, all of actions(Up, Down, Left, Right)]\n",
    "        #np.random.randn : Return a sample (or samples) from the “standard normal” distribution.\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action) #Get new state and reward from environment\n",
    "        Q[state, action] = reward + np.max(Q[new_state, :]) #Update table with new knowledge using learning rate\n",
    "        rAll += reward #Sum up the reward.\n",
    "        state = new_state #state update (the agent moves to next state)\n",
    "\n",
    "    #After the game is over. (while loop finished)\n",
    "    rList.append(rAll) #Append result to the list.   \n",
    "\n",
    "print(\"Success rate: \" + str(sum(rList) / num_episodes))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)\n",
    "plt.bar(range(len(rList)), rList, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the map that updated all the states.\n",
    "\n",
    "![frozen_lake_with_EE](images/frozen_lake_with_EE.png)\n",
    "\n",
    "But, this algorithm has some problems too. If the agent is on $s_{10}$, where should the agent go?\n",
    "\n",
    "![where_will_you_go](images/where_will_you_go.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 4. Optimized Q-table with Discounted reward\n",
    "\n",
    "So, we need a solution. The problem with our algorithm is that all the reward is 1. There are two reward in this game: present reward and future reward. However, our algorithm doesn't distinguish between them. So, we should update our algorithm to distinguish current reward and future reward. We will give a weight to the present reward. \n",
    "Getting immediate reward is better than delayed reward. Here is our new Q-learning equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is weight. In this case, $r$ is present reward and $\\max{Q(s', a')}$ is future reward. Because we need reduce future reward, $\\gamma$ should be less than 1. Let's update the map again using this formula when $\\gamma$ is 0.9.\n",
    "\n",
    "![discounted_reward_1](images/discounted_reward_1.png)\n",
    "\n",
    "$s_{15}$ is our goal. So, $Q(s_{14}, a_{right})$ is 1. \n",
    "\n",
    "![discounted_reward_2](images/discounted_reward_2.png)\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(s_{13}, a) = 0 + 0.9\\cdot{Q(s_{14}, a_{right})} = 0.9\n",
    "$$\n",
    "\n",
    "![discounted_reward_3](images/discounted_reward_3.png)\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(s_{9}, a) = 0 + 0.9\\cdot{Q(s_{13}, a_{right})} = 0.9\\cdot0.9 = 0.81\n",
    "$$\n",
    "\n",
    "![discounted_reward_4](images/discounted_reward_4.png)\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(s_{10}, a) = 0 + 0.9\\cdot{Q(s_{9}, a_{down})} = 0.9\\cdot0.9\\cdot0.9 = 0.72\n",
    "$$\n",
    "\n",
    "Like this way, we can update $Q(s_{10}, a_{down})$.\n",
    "\n",
    "![discounted_reward_5](images/discounted_reward_5.png)\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(s_{10}, a) = 0 + 0.9\\cdot{Q(s_{14}, a_{right})} = 0.9\n",
    "$$\n",
    "\n",
    "Finally, the agent can distinguish optimized route. On $s_{10}$, the agent will choose $a_{down}$.\n",
    "\n",
    "![discounted_reward_6](images/discounted_reward_6.png)\n",
    "\n",
    "In conclusion, we can summarize our final algorithm.\n",
    "\n",
    "* For each $s$, $a$ initialize table entry $\\hat{Q}(s, a) \\leftarrow 0$. \n",
    "* Observe current state $s$\n",
    "* Do forever:\n",
    "    * Select an action $a$ and execute it\n",
    "    * Receive immediate reward $r$\n",
    "    * Observe the new state $s'$\n",
    "    * Update the table entry for $\\hat{Q}(s, a)$ as follows:        \n",
    "        $\\hat{Q}(s, a) \\leftarrow r + \\gamma\\max\\hat{Q}(s', a')$\n",
    "    * $s \\leftarrow s'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.9555\n",
      "Final Q-Table Values\n",
      "[[ 0.          0.          0.95099005  0.        ]\n",
      " [ 0.          0.          0.96059601  0.        ]\n",
      " [ 0.          0.970299    0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.9801      0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.99        0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.99        0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJalBhrskNchwl6QG/T+YjEVAv2QhFwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c8caef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n]) #16, 4(4x4 : each state, [Up, Down, Left, Right])\n",
    "\n",
    "# Discount factor\n",
    "dis = .99\n",
    "\n",
    "#Set learning parameters\n",
    "num_episodes = 2000 #Repeat 2000 times\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "rList = [] #List for saving result\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # Reset environment and get first new observation\n",
    "    state = env.reset() #Initialize evironment \n",
    "    rAll = 0 #Result. When this value is 0, it means the agent fall in a hole. \n",
    "             #And when this value is 1, it means the agent reach the goal.\n",
    "    done = False \n",
    "\n",
    "    # The Q-Table learning algorithm\n",
    "    while not done:\n",
    "        # Choose an action by greedily (with noise) picking from Q table\n",
    "        action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i + 1)) #Add random noise.\n",
    "        #parameter : Q[current state, all of actions(Up, Down, Left, Right)]\n",
    "        #np.random.randn : Return a sample (or samples) from the “standard normal” distribution.\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action) #Get new state and reward from environment\n",
    "        Q[state, action] = reward + dis * np.max(Q[new_state, :]) #Update table with new knowledge using decay rate\n",
    "        rAll += reward #Sum up the reward.\n",
    "        state = new_state #state update (the agent moves to next state)\n",
    "\n",
    "    #After the game is over. (while loop finished)\n",
    "    rList.append(rAll) #Append result to the list.   \n",
    "\n",
    "print(\"Success rate: \" + str(sum(rList) / num_episodes))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)\n",
    "plt.bar(range(len(rList)), rList, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, here is the reward map.\n",
    "\n",
    "![dummy](images/dummy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 5. Q-table in real world\n",
    "\n",
    "Like the name of the game 'Frozenlake', let's think we're on an real frozen lake. The place will be very slippery. So, although we should move carefully, we couldn't move accurately. Fon instance, we tried to move to right, but we could slip down. In previous environment, the agent could move accurately. Because it is artificial environment. But we can set more realistic environment. We call this environment as **Stochastic model**.\n",
    "\n",
    "* **Deterministic models** : the output of the model is fully determined by the parameter values and the initial conditions initial conditions.\n",
    "* **Stochastic models** : possess some inherent randomness. The same set of parameter values and initial conditions will lead to an ensemble of different outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Cannot re-register id: FrozenLake-v4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-67ff07f2c88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mentry_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gym.envs.toy_text:FrozenLakeEnv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     kwargs={'map_name': '4x4',\n\u001b[0;32m----> 5\u001b[0;31m             'is_slippery': True} #changing is_slippery value.\n\u001b[0m\u001b[1;32m      6\u001b[0m ) #Environment setting\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, id, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot re-register id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Cannot re-register id: FrozenLake-v4"
     ]
    }
   ],
   "source": [
    "register(\n",
    "    id='FrozenLake-v2',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4',\n",
    "            'is_slippery': True} #changing is_slippery value.\n",
    ") #Environment setting\n",
    "\n",
    "env = gym.make(\"FrozenLake-v2\") #Environment generate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make Stochastic model by changing 'is_slippery' value to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    env.render() #Display environment\n",
    "    action = env.action_space.sample() #The agent action.\n",
    "    observation, reward, done, info = env.step(action) #Execute step and get infomations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to results, The agent doesn't move as we expected. Let's apply our previous algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.018\n",
      "Final Q-Table Values\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a45a358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n]) #16, 4(4x4 : each state, [Up, Down, Left, Right])\n",
    "\n",
    "# Discount factor\n",
    "dis = .99\n",
    "\n",
    "#Set learning parameters\n",
    "num_episodes = 2000 #Repeat 2000 times\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "rList = [] #List for saving result\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # Reset environment and get first new observation\n",
    "    state = env.reset() #Initialize evironment \n",
    "    rAll = 0 #Result. When this value is 0, it means the agent fall in a hole. \n",
    "             #And when this value is 1, it means the agent reach the goal.\n",
    "    done = False \n",
    "\n",
    "    # The Q-Table learning algorithm\n",
    "    while not done:\n",
    "        # Choose an action by greedily (with noise) picking from Q table\n",
    "        action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i + 1)) #Add random noise.\n",
    "        #parameter : Q[current state, all of actions(Up, Down, Left, Right)]\n",
    "        #np.random.randn : Return a sample (or samples) from the “standard normal” distribution.\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action) #Get new state and reward from environment\n",
    "        Q[state, action] = reward + dis * np.max(Q[new_state, :]) #Update table with new knowledge using decay rate\n",
    "        rAll += reward #Sum up the reward.\n",
    "        state = new_state #state update (the agent moves to next state)\n",
    "\n",
    "    #After the game is over. (while loop finished)\n",
    "    rList.append(rAll) #Append result to the list.   \n",
    "\n",
    "print(\"Success rate: \" + str(sum(rList) / num_episodes))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)\n",
    "plt.bar(range(len(rList)), rList, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Totally failed. We need new solution. The solution is simple:\n",
    "> Two heads are better than one\n",
    "\n",
    "Like our life mentors. Don’t just listen and follow one mentor. we need our own solutions. In previour model, we just accept Q-function completely. So, our new algorithm is that listen to $Q(s')$ just little bit(mentor's solution) and update $Q(s)$ little bit(our own solution). So, we will update $Q(s)$ by learning rate. \n",
    "\n",
    "There is our previous equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow r + \\gamma \\max{Q(s', a')}\n",
    "$$\n",
    "\n",
    "And Here is a new equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow (1-\\alpha){Q(s, a)} + \\alpha[r + \\gamma \\max{Q(s', a')}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow {Q(s, a)} + \\alpha[r + \\gamma \\max{Q(s', a')} - {Q(s, a)}]\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is learning rate. And, let's update our algorithm.\n",
    "\n",
    "* For each $s$, $a$ initialize table entry $\\hat{Q}(s, a) \\leftarrow 0$. \n",
    "* Observe current state $s$\n",
    "* Do forever:\n",
    "    * Select an action $a$ and execute it\n",
    "    * Receive immediate reward $r$\n",
    "    * Observe the new state $s'$\n",
    "    * Update the table entry for $\\hat{Q}(s, a)$ as follows:        \n",
    "        $Q(s, a) \\leftarrow (1-\\alpha){Q(s, a)} + \\alpha[r + \\gamma \\max{Q(s', a')}]$\n",
    "    * $s \\leftarrow s'$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.5285\n",
      "Final Q-Table Values\n",
      "[[  3.63002867e-01   4.08257551e-03   3.73545590e-03   1.36662716e-03]\n",
      " [  5.54826485e-03   7.09606095e-04   7.15699305e-05   4.86682365e-01]\n",
      " [  3.46632231e-01   8.86687841e-05   1.52845523e-03   3.79841329e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  5.69934871e-01   8.82252814e-04   9.92675595e-04   1.16721890e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.17954562e-03   2.57846359e-01   3.89733149e-05   4.56718132e-15]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  3.01625217e-03   5.75088809e-05   0.00000000e+00   7.60906185e-01]\n",
      " [  0.00000000e+00   9.22836259e-01   0.00000000e+00   2.54932819e-04]\n",
      " [  6.80365390e-01   6.58116684e-04   5.45639233e-04   3.42339866e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   3.26543137e-03   9.79755385e-01   9.87929348e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   9.99197921e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nji+hAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c0da550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n]) #16, 4(4x4 : each state, [Up, Down, Left, Right])\n",
    "\n",
    "# Discount factor\n",
    "dis = .99\n",
    "\n",
    "#Set learning parameters\n",
    "num_episodes = 2000 #Repeat 2000 times\n",
    "\n",
    "#Set learning rate\n",
    "learning_rate = .85\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "rList = [] #List for saving result\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # Reset environment and get first new observation\n",
    "    state = env.reset() #Initialize evironment \n",
    "    rAll = 0 #Result. When this value is 0, it means the agent fall in a hole. \n",
    "             #And when this value is 1, it means the agent reach the goal.\n",
    "    done = False \n",
    "\n",
    "    # The Q-Table learning algorithm\n",
    "    while not done:\n",
    "        # Choose an action by greedily (with noise) picking from Q table\n",
    "        action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i + 1)) #Add random noise.\n",
    "        #parameter : Q[current state, all of actions(Up, Down, Left, Right)]\n",
    "        #np.random.randn : Return a sample (or samples) from the “standard normal” distribution.\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action) #Get new state and reward from environment\n",
    "        # Update Q-Table with new knowledge using learning rate\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] + learning_rate * (reward + dis * np.max(Q[new_state, :]))\n",
    "        rAll += reward #Sum up the reward.\n",
    "        state = new_state #state update (the agent moves to next state)\n",
    "\n",
    "    #After the game is over. (while loop finished)\n",
    "    rList.append(rAll) #Append result to the list.   \n",
    "\n",
    "print(\"Success rate: \" + str(sum(rList) / num_episodes))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)\n",
    "plt.bar(range(len(rList)), rList, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to result, after learning, we can see that the agent usually succeeds even though it is in stochastic environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 6. Q-Network\n",
    "\n",
    "We fully understood Q-learning! But think about it. Can we apply this model to real life or more complex environment? Maybe not. In previous environment, we just used 16 states and 4 actions. Even if you simply represent an image of 16 * 16 pixels by RGB, we should prepare $256^{16\\cdot16}$ tables. So, we need more universal model: **Neural Network**.\n",
    "\n",
    "![q_network](images/q_network.png)\n",
    "\n",
    "Compared with the Q-table, The Q-network receives the state and returns all possible action values. In previous Frozenlake game, Let's apply the Q-network by example state:\n",
    "\n",
    "$$\n",
    "Q_{network}(s_{example}, a) = [0.5, 0.1, 0.0, 0.8]\n",
    "$$\n",
    "\n",
    "$[0.5, 0.1, 0.0, 0.8]$ represents $(a_{left}, a_{right}, a_{up}, a_{down})$.\n",
    "\n",
    "![nn](images/nn.png)\n",
    "\n",
    "In the Neural Network, when we put state into the input, we will receive reward as output.\n",
    "\n",
    "$$\n",
    "H(x) = Wx = \\hat{Q}(s,a)\n",
    "$$\n",
    "\n",
    "where $H$ is hypothesis, $W$ is ouput of the Neural Network. So, we can calculate cost by [Mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error).\n",
    "\n",
    "$$\n",
    "cost(W) = \\frac{1}{n}\\sum_{i=1}^{n} (Wx_i - y_i)^2\n",
    "$$\n",
    "\n",
    "where $y$ is taget label. In Q-learning, we don't have any target label because it is reinforcement learning. So, in this case, we will set target label as $Q(s, a)$. Because optimize $Q(s, a)$ is our goal.\n",
    "\n",
    "$$\n",
    "cost(W) = \\frac{1}{n}\\sum_{i=1}^{n} (Wx_i - y_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "cost(W) = (Wx - y)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y =  r + \\gamma\\max{Q(s')}\n",
    "$$\n",
    "\n",
    "Let's approximate $Q^*$ function using weight:\n",
    "$$\n",
    "\\hat{Q}(s, a|\\theta) \\to {Q}^*(s, a)\n",
    "$$\n",
    "\n",
    "where $\\theta$ is weight. So, we try to optimize Q-function by using $s$, $a$ and $\\theta$. So, we can choose $\\theta$ to minimize using this equation:\n",
    "\n",
    "$$\n",
    "\\min\\sum_{t=0}^{T}[\\hat{Q}(s_t, a_t|\\theta) - (r_t + \\gamma\\max\\hat{Q}(s_{t+1}, a'|\\theta))]^2\n",
    "$$\n",
    "\n",
    "It looks complicated. But it is just same as optimizing weight in Linear regression by Mean squared error.\n",
    "\n",
    "Putting all this together, we can list out the algorithm we'll use to train the network.\n",
    "\n",
    "* Initialize action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "    Initialise sequence $s_1$ = {$x_1$} and preprocessed sequence $\\phi_1 = \\phi(s_1)$ \n",
    "  * **For** $t=1$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\max_a Q^*(\\phi(s_t), a;\\theta)$\n",
    "     * Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$\n",
    "     * Set $s_{t+1}$ = $s_t, a_t, x_{t+1}$ and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$\n",
    "     * Set$\\;{y_j} = \\left\\{\\begin{matrix}& r_{j}\\;(for\\;terminal\\;\\phi_{j+1})\\\\ & r_j + \\gamma \\max_{a'}{Q(\\phi_{j+1}, a';\\theta)} \\;(for\\;non-terminal\\;\\phi_{j+1})\\end{matrix}\\right.$\n",
    "     * Perform a gradient descent step on $(y_j - Q(\\phi_j, a_j;\\theta))^2$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    return np.identity(16)[x: x+1]\n",
    "    #np.identity로 one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make one_hot encoding easier, we can make helper method. [np.identity](https://docs.scipy.org/doc/numpy/reference/generated/numpy.identity.html) will return the identity array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of successful episodes: 0.5355%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129a28cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Input and output size based on the Env\n",
    "input_size = env.observation_space.n # 16 (4x4)\n",
    "output_size = env.action_space.n # 4actions (Up, Down, Left, Right)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# These lines establish the feed-forward part of the network used to\n",
    "# choose actions\n",
    "X = tf.placeholder(shape=[1, input_size], dtype=tf.float32) # state input\n",
    "W = tf.Variable(tf.random_uniform([input_size, output_size], 0, 0.01)) # weight\n",
    "\n",
    "Qpred = tf.matmul(X, W)  # Out Q prediction\n",
    "Y = tf.placeholder(shape=[1, output_size], dtype=tf.float32) # Y label\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpred)) \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Set Q-learning related parameters\n",
    "dis = .99\n",
    "num_episodes = 2000\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "rList = []\n",
    "init = tf.global_variables_initializer() #tf initialize\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        # Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        e = 1. / ((i / 50) + 10)\n",
    "        rAll = 0\n",
    "        done = False\n",
    "        local_loss = []\n",
    "\n",
    "        # The Q-Network training\n",
    "        while not done:\n",
    "            # Choose an action by greedily (with e chance of random action)\n",
    "            # from the Q-network\n",
    "            Qs = sess.run(Qpred, feed_dict={X: one_hot(s)})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = np.argmax(Qs)\n",
    "\n",
    "            # Get new state and reward from environment\n",
    "            s1, reward, done, _ = env.step(a) \n",
    "            if done:\n",
    "                # Update Q, and no Qs+1, since it's a terminal state\n",
    "                Qs[0, a] = reward #When the agent got a reward, update Q\n",
    "            else:\n",
    "                # Obtain the Q_s1 values by feeding the new state through our\n",
    "                # network\n",
    "                Qs1 = sess.run(Qpred, feed_dict={X: one_hot(s1)}) #Continue learning\n",
    "                # Update Q\n",
    "                Qs[0, a] = reward + dis * np.max(Qs1)\n",
    "\n",
    "            # Train our network using target (Y) and predicted Q (Qpred) values\n",
    "            sess.run(train, feed_dict={X: one_hot(s), Y: Qs})\n",
    "\n",
    "            rAll += reward\n",
    "            s = s1 #move to next state\n",
    "        rList.append(rAll)\n",
    "\n",
    "print(\"Percent of successful episodes: \" +\n",
    "      str(sum(rList) / num_episodes) + \"%\")\n",
    "plt.bar(range(len(rList)), rList, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But actually this model isn't efficient. In other words, it has some problem. How can we fix it? The solution is **DQN**. But, before we update our model with DQN, let's look at other game using same model. The other game is **CartPole**. In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![cart_pole](images/cart_pole.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-06 22:36:24,941] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0') #Load CartPole environment\n",
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through env. To show the simulation running, you can use env.render() to render one frame. Passing in an action as an integer to env.step will generate the next step in the simulation. You can see how many actions are possible from env.action_space and to get a random action you can use env.action_space.sample(). This is general to all Gym games. In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to watch the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for this episode 1 was:20.0\n",
      "Reward for this episode 2 was:18.0\n",
      "Reward for this episode 3 was:16.0\n",
      "Reward for this episode 4 was:19.0\n",
      "Reward for this episode 5 was:17.0\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    state, reward, done, info = env.step(env.action_space.sample()) #Take a random action\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        rewards = []\n",
    "        print(\"Reward for this episode {} was: {}\".format(random_episodes, reward_sum))\n",
    "        reward_sum = 0\n",
    "        env.reset()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut the window showing the simulation, use env.close().\n",
    "\n",
    "If you ran the simulation above, we can look at the rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(rewards[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each frame while the simulation is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right. So, let's make network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable W1 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-5-3bbf03a0a27c>\", line 11, in <module>\n    W1 = tf.get_variable(\"W1\", shape=[input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n  File \"/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1fb43363d3b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# First layer of weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mQpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable W1 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-5-3bbf03a0a27c>\", line 11, in <module>\n    W1 = tf.get_variable(\"W1\", shape=[input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n  File \"/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Constants defining our neural network\n",
    "learning_rate = 1e-1\n",
    "input_size = env.observation_space.shape[0] #4\n",
    "output_size = env.action_space.n #2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_size], name=\"input_x\")\n",
    "\n",
    "# First layer of weights\n",
    "W1 = tf.get_variable(\"W1\", shape=[input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "Qpred = tf.matmul(X, W1)\n",
    "\n",
    "# We need to define the parts of the network needed for learning a policy\n",
    "Y = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpred))\n",
    "# Learning\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very simillar with previous 'FrozenLake' environment. Like this, we can make network by Q-network. If there are differences, we will receive left or right as Q-reward. In 'FrozenLake' environment, we received up, down, left and right as Q-reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Values for q learning\n",
    "max_episodes = 5000\n",
    "dis = 0.9\n",
    "step_history = []\n",
    "\n",
    "# Setting up our environment\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    e = 1. / ((episode / 10) + 1)\n",
    "    step_count = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # The Q-Network training\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "        x = np.reshape(state, [1, input_size])\n",
    "        # Choose an action by greedily (with e chance of random action) from\n",
    "        # the Q-network\n",
    "        Q = sess.run(Qpred, feed_dict={X: x})\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q)\n",
    "\n",
    "        # Get new state and reward from environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            Q[0, action] = -100\n",
    "            #If the pole has fallen, the agent will get negative reward and the game will be over.\n",
    "            #We set negative 100 reward in this situation.\n",
    "        else:\n",
    "            x_next = np.reshape(next_state, [1, input_size])\n",
    "            # Obtain the Q' values by feeding the new state through our network\n",
    "            Q_next = sess.run(Qpred, feed_dict={X: x_next})\n",
    "            Q[0, action] = reward + dis * np.max(Q_next)\n",
    "\n",
    "        # Train our network using target and predicted Q values on each episode\n",
    "        sess.run(train, feed_dict={X: x, Y: Q}) #We put optimized Q as target label at current state\n",
    "        state = next_state\n",
    "\n",
    "    step_history.append(step_count)\n",
    "    print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
    "    # If last 10's avg steps are 500, it's good enough\n",
    "    if len(step_history) > 10 and np.mean(step_history[-10:]) > 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the code after env.step()\n",
    "\n",
    "    if done:\n",
    "            Q[0, action] = -100\n",
    "        else:\n",
    "            x_next = np.reshape(next_state, [1, input_size])\n",
    "            Q_next = sess.run(Qpred, feed_dict={X: x_next})\n",
    "            Q[0, action] = reward + dis * np.max(Q_next)\n",
    "\n",
    "It follows our algorithm:\n",
    "\n",
    "$$\n",
    "Set\\;{y_i} = \n",
    "\\left\\{\\begin{matrix}\n",
    "& r_{j}\\;(for\\;terminal)\\\\ \n",
    "& r_{j} + \\gamma\\max{Q} \\;(for\\;non-terminal)\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "If the pole has fallen, the agent will get negative reward and the episode will be over. So, we set negative 100 reward(big penalty) in this situation. By doing this, the agent will be train.\n",
    "\n",
    "    if len(step_history) > 10 and np.mean(step_history[-10:]) > 500:\n",
    "        break\n",
    "\n",
    "If the pole doesn't have fall over 500 steps, we will consider that the agent is learned.\n",
    "\n",
    "After training, we can use our model as test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f2f8eb765a27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# See our trained network in action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreward_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# See our trained network in action\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    x = np.reshape(observation, [1, input_size])\n",
    "    Q = sess.run(Qpred, feed_dict={X: x})\n",
    "    action = np.argmax(Q)\n",
    "\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "But, our network doesn't work well. Because our model has some problems. But, how can we fix our model? The answer is that: use **Deep Q-Network**. Finally, it is time to implement DQN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 7. Deep Q-network\n",
    "\n",
    "Our model has three main problems:\n",
    "\n",
    "* Too shallow\n",
    "* Correlations between samples\n",
    "* Non-stationary targets\n",
    "    \n",
    "First, we use only one layer. It isn't deep enough to make a good solution.\n",
    "\n",
    "And second, it has problem that **Correlations between samples**. According to algorithm, each data that we received is very simmilar. Because when we received reward, the pole state isn't significantly different from previous state. So, there are correlations between samples. But, why is it a problem? \n",
    "\n",
    "![DQN_linear_1.png](images/DQN_linear_1.png)\n",
    "\n",
    "This is a graph of weight and mile per gallon about a car. And we can draw a blue line as a result of linear regression. But if we had only a few correlational data like in the black circle, we would draw different line as a red. It makes totally different result. Our previous model has same problem. The agent could be trained incorrectly because it was just trained by correlational samples. \n",
    "\n",
    "Third problem is **Non-stationary targets**. Let's check our cost formula again.\n",
    "\n",
    "$$\n",
    "\\min\\sum_{t=0}^{T}[\\hat{Q}(s_t, a_t|\\theta) - (r_t + \\gamma\\max\\hat{Q}(s_{t+1}, a'|\\theta))]^2\n",
    "$$\n",
    "\n",
    "In this equation, $\\min\\sum_{t=0}^{T} \\hat{Q}(s_t, a_t|\\theta)$ is our prediction. And $(r_t + \\gamma\\max\\hat{Q}(s_{t+1}, a'|\\theta))^2$ is our target.\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\hat{Q}(s_t, a_t|\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y = (r_t + \\gamma\\max\\hat{Q}(s_{t+1}, a'|\\theta)\n",
    "$$\n",
    "\n",
    "So, our goal is making $\\hat{Y}$ and $Y$ similar. But, we have same $\\theta$ in each equation. When we update our $\\hat{Y}$, our target $Y$ is updated too. It is like that immediately moving a target after shooting an arrow. \n",
    "This is an ironical situation. So, we call this situation **Non-stationary targets**.\n",
    "\n",
    "So, we have solutions:\n",
    "* Too shallow\n",
    "    * Go deep\n",
    "* Correlations between samples\n",
    "    * Capture and replay\n",
    "* Non-stationary targets\n",
    "    * Separate networks: Create a target network\n",
    "\n",
    "And we call this solution network **Deep Q-network**. And let's dive into DQN's solutions.\n",
    "\n",
    "First, **Go deep**. Actually our previous network isn't deeper enough. So, we should make our network deeper as making more layers. \n",
    "\n",
    "![Deep Q-Learning Atari](images/atari-network.png)\n",
    "\n",
    "Second, **Capture and replay**. This is solution of Correlations between samples problem. This concept is very simple. To prevent Correlations between samples, we save data in buffer after the agent's action. Then choose random sample in the buffer and update by algorithm.\n",
    "\n",
    "![capture_and_replay](images/capture_and_replay.png)\n",
    "\n",
    "Let's see the graph of weight and mile per gallon about a car again.\n",
    "\n",
    "![DQN_linear_2](images/DQN_linear_2.png)\n",
    "\n",
    "In this graph, circles are same as previous scatter graph. And blue line is linear regression line by circles. However I choose half of previous samples ramdomly, and mark as 'x'. And red line is linear regression line by 'x's. As you can see, each result are almost same. Like this, we can train our Q-network by choosing random samples in buffer.\n",
    "\n",
    "So, we can summarize that:\n",
    "\n",
    "* Initialize action-value network $Q$ with random weights\n",
    "* **For** episode = 1, $M$ **do**\n",
    "    Initialise sequence $s_1$ = {$x_1$} and preprocessed sequence $\\phi_1 = \\phi(s_1)$ \n",
    "  * **For** $t=1$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\max_a Q^*(\\phi(s_t), a;\\theta)$\n",
    "     * Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$\n",
    "     * Set $s_{t+1}$ = $s_t, a_t, x_{t+1}$ and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$\n",
    "     * **Store transition $(\\phi_t, a_t, r_t, \\phi_{t+1})$ in $D$**\n",
    "     * **Sample random minibatch of transition $(\\phi_j, a_j, r_j, \\phi_{j+1})$ from $D$**\n",
    "     * Set$\\;{y_j} = \\left\\{\\begin{matrix}& r_{j}\\;(for\\;terminal\\;\\phi_{j+1})\\\\ & r_j + \\gamma \\max_{a'}{Q(\\phi_{j+1}, a';\\theta)} \\;(for\\;non-terminal\\;\\phi_{j+1})\\end{matrix}\\right.$\n",
    "     * Perform a gradient descent step on $(y_j - Q(\\phi_j, a_j;\\theta))^2$\n",
    "  * **endfor**\n",
    "* **endfor**\n",
    "\n",
    "Third, **Separate networks: Create a target network**. This is solution of Non-stationary targets problem. It is really simple. Just create two network. Let's update our equation.\n",
    "\n",
    "$$\n",
    "\\min\\sum_{t=0}^{T}[\\hat{Q}(s_t, a_t|\\theta) - (r_t + \\gamma\\max\\hat{Q}(s_{t+1}, a'|\\bar\\theta))]^2\n",
    "$$\n",
    "\n",
    "Only one difference is $\\bar\\theta$ in target label. We will separate network as training and target. First of all, we will update only training network. And after several times, update target network too. In our algorithm, this solution corresponds to\n",
    "\n",
    "* Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "\n",
    "We will update $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$ by another network.\n",
    "\n",
    "Finally, we can summarize our algorithm as DQN:\n",
    "\n",
    "* Initialize replay memory $D$ to capacity $N$\n",
    "* Initialize action-value network $Q$ with random weights $\\theta$\n",
    "* Initialize target action-value network $\\hat{Q}$ with random weights $\\bar\\theta = \\theta$\n",
    "* **For** episode = 1, $M$ **do**\n",
    "    Initialise sequence $s_1$ = {$x_1$} and preprocessed sequence $\\phi_1 = \\phi(s_1)$ \n",
    "  * **For** $t=1$, $T$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\max_a Q^*(\\phi(s_t), a;\\theta)$\n",
    "     * Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$\n",
    "     * Set $s_{t+1}$ = $s_t, a_t, x_{t+1}$ and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$\n",
    "     * Store transition $(\\phi_t, a_t, r_t, \\phi_{t+1})$ in $D$\n",
    "     * Sample random minibatch of transition $(\\phi_j, a_j, r_j, \\phi_{j+1})$ from $D$\n",
    "     * Set$\\;{y_j} = \\left\\{\\begin{matrix}& r_{j}\\;(if\\;episode\\;teminates\\;at\\;step\\;{j+1})\\\\ & r_j + \\gamma \\max_{a'}{\\hat{Q}(\\phi_{j+1}, a';\\bar\\theta)} \\;(otherwise)\\end{matrix}\\right.$\n",
    "     * Perform a gradient descent step on $(y_j - Q(\\phi_j, a_j;\\theta))^2$ with respect to the network $\\theta$\n",
    "     * Every $C$ steps reset $\\hat{Q} = Q$\n",
    "  * **endfor**\n",
    "* **endfor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    def __init__(self, session: tf.Session, input_size: int, output_size: int, name: str=\"main\") -> None:\n",
    "        \"\"\"DQN Agent can\n",
    "        1) Build network\n",
    "        2) Predict Q_value given state\n",
    "        3) Train parameters\n",
    "        Args:\n",
    "            session (tf.Session): Tensorflow session\n",
    "            input_size (int): Input dimension\n",
    "            output_size (int): Number of discrete actions\n",
    "            name (str, optional): TF Graph will be built under this name scope\n",
    "        \"\"\"\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "\n",
    "        self._build_network()\n",
    "\n",
    "    def _build_network(self, h_size=16, l_rate=0.001) -> None:\n",
    "        \"\"\"DQN Network architecture (simple MLP)\n",
    "        Args:\n",
    "            h_size (int, optional): Hidden layer dimension\n",
    "            l_rate (float, optional): Learning rate\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
    "            net = self._X\n",
    "\n",
    "            net = tf.layers.dense(net, h_size, activation=tf.nn.relu)\n",
    "            net = tf.layers.dense(net, self.output_size)\n",
    "            self._Qpred = net\n",
    "\n",
    "            self._Y = tf.placeholder(tf.float32, shape=[None, self.output_size])\n",
    "            self._loss = tf.losses.mean_squared_error(self._Y, self._Qpred)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=l_rate)\n",
    "            self._train = optimizer.minimize(self._loss)\n",
    "\n",
    "    def predict(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Returns Q(s, a)\n",
    "        Args:\n",
    "            state (np.ndarray): State array, shape (n, input_dim)\n",
    "        Returns:\n",
    "            np.ndarray: Q value array, shape (n, output_dim)\n",
    "        \"\"\"\n",
    "        x = np.reshape(state, [-1, self.input_size])\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X: x})\n",
    "\n",
    "    def update(self, x_stack: np.ndarray, y_stack: np.ndarray) -> list:\n",
    "        \"\"\"Performs updates on given X and y and returns a result\n",
    "        Args:\n",
    "            x_stack (np.ndarray): State array, shape (n, input_dim)\n",
    "            y_stack (np.ndarray): Target Q array, shape (n, output_dim)\n",
    "        Returns:\n",
    "            list: First element is loss, second element is a result from train step\n",
    "        \"\"\"\n",
    "        feed = {\n",
    "            self._X: x_stack,\n",
    "            self._Y: y_stack\n",
    "        }\n",
    "        return self.session.run([self._loss, self._train], feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-09 17:55:14,015] Making new env: CartPole-v0\n",
      "[2017-08-09 17:55:14,022] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/geunseong-gai/Documents/Python/Udacity/MLND/capstone/gym-results')\n",
      "[2017-08-09 17:55:14,024] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "from typing import List\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.wrappers.Monitor(env, directory=\"gym-results/\", force=True)\n",
    "\n",
    "# Constants defining our neural network\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "REPLAY_MEMORY = 50000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQUENCY = 5\n",
    "MAX_EPISODES = 5000\n",
    "\n",
    "\n",
    "def replay_train(mainDQN: DQN, targetDQN: DQN, train_batch: list) -> float:\n",
    "    \"\"\"Trains `mainDQN` with target Q values given by `targetDQN`\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): Main DQN that will be trained\n",
    "        targetDQN (dqn.DQN): Target DQN that will predict Q_target\n",
    "        train_batch (list): Minibatch of replay memory\n",
    "            Each element is (s, a, r, s', done)\n",
    "            [(state, action, reward, next_state, done), ...]\n",
    "    Returns:\n",
    "        float: After updating `mainDQN`, it returns a `loss`\n",
    "    \"\"\"\n",
    "    states = np.vstack([x[0] for x in train_batch])\n",
    "    actions = np.array([x[1] for x in train_batch])\n",
    "    rewards = np.array([x[2] for x in train_batch])\n",
    "    next_states = np.vstack([x[3] for x in train_batch])\n",
    "    done = np.array([x[4] for x in train_batch])\n",
    "\n",
    "    X = states\n",
    "\n",
    "    Q_target = rewards + DISCOUNT_RATE * np.max(targetDQN.predict(next_states), axis=1) * ~done\n",
    "\n",
    "    y = mainDQN.predict(states)\n",
    "    y[np.arange(len(X)), actions] = Q_target\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    return mainDQN.update(X, y)\n",
    "\n",
    "\n",
    "def get_copy_var_ops(*, dest_scope_name: str, src_scope_name: str) -> List[tf.Operation]:\n",
    "    \"\"\"Creates TF operations that copy weights from `src_scope` to `dest_scope`\n",
    "    Args:\n",
    "        dest_scope_name (str): Destination weights (copy to)\n",
    "        src_scope_name (str): Source weight (copy from)\n",
    "    Returns:\n",
    "        List[tf.Operation]: Update operations are created and returned\n",
    "    \"\"\"\n",
    "    # Copy variables src_scope to dest_scope\n",
    "    op_holder = []\n",
    "\n",
    "    src_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    dest_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
    "\n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def bot_play(mainDQN: DQN, env: gym.Env) -> None:\n",
    "    \"\"\"Test runs with rendering and prints the total score\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): DQN agent to run a test\n",
    "        env (gym.Env): Gym Environment\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(reward_sum))\n",
    "            break\n",
    "\n",
    "\n",
    "def main():\n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
    "\n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"main\")\n",
    "        targetDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"target\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # initial copy q_net -> target_net\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name=\"target\",\n",
    "                                    src_scope_name=\"main\")\n",
    "        sess.run(copy_ops)\n",
    "\n",
    "        for episode in range(MAX_EPISODES):\n",
    "            e = 1. / ((episode / 10) + 1)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # Choose an action by greedily from the Q-network\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "                # Get new state and reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if done:  # Penalty\n",
    "                    reward = -1\n",
    "\n",
    "                # Save the experience to our buffer\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                if len(replay_buffer) > BATCH_SIZE:\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                    loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n",
    "\n",
    "                if step_count % TARGET_UPDATE_FREQUENCY == 0:\n",
    "                    sess.run(copy_ops)\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "            print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
    "\n",
    "            # CartPole-v0 Game Clear Checking Logic\n",
    "            last_100_game_reward.append(step_count)\n",
    "\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "\n",
    "                if avg_reward > 199:\n",
    "                    print(f\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
